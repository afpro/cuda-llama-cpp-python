# LLama

[llama-cpp-python](https://github.com/abetlen/llama-cpp-python) container with cuda support

based on [3x3cut0r/llama-cpp-python](https://github.com/3x3cut0r/docker/blob/main/llama-cpp-python)
